# Performance Benchmarks

Comparison of `PGJsonbStorage` vs `RelStorage` (both using PostgreSQL)
for ZODB storage operations at multiple abstraction levels.

Measured on: 2026-02-08
Python: 3.13.9, PostgreSQL: 17, 100 iterations, 10 warmup
Host: localhost, Docker-containerized PostgreSQL

## Architecture Differences

| | PGJsonbStorage | RelStorage |
|---|---|---|
| Storage format | JSONB (queryable via SQL) | bytea (opaque pickle) |
| Load cache | OrderedDict LRU (Python), 64 MB, per-instance | Generational LRU (Cython), 10 MB, shared |
| Cache miss cost | SQL SELECT + JSONB→pickle transcode (Rust) | SQL SELECT (raw bytes) |
| Store cost | pickle→JSONB transcode (Rust) + INSERT | raw pickle INSERT |
| Pack/GC | Pure SQL recursive CTE (refs column) | Load + unpickle every object |

## Storage API (raw store/load)

### Writes

| Operation | PGJsonb | RelStorage | Comparison |
|---|---|---|---|
| store single | 3.6 ms | 4.8 ms | **1.3x faster** |
| store batch 10 | 4.8 ms | 10.4 ms | **2.2x faster** |
| store batch 100 | 7.8 ms | 6.5 ms | 1.2x slower |

Single and small-batch stores are faster because PGJsonbStorage has a simpler
2PC path (direct SQL, no OID/TID tracking tables). Batch-100 stores are
slightly slower due to PostgreSQL JSONB indexing overhead vs raw bytea writes.
Transcoding cost is <1% of batch time (the Rust codec processes 100 objects
in ~0.14 ms via `decode_zodb_record_for_pg`).

### Loads (cached — in-memory LRU hit)

| Operation | PGJsonb | RelStorage | Comparison |
|---|---|---|---|
| load cached | <1 us | 1 us | **3.7x faster** |
| load batch cached (100) | 24 us | 105 us | **4.4x faster** |

Cached loads measure in-memory cache lookup speed. Both storages serve hot
objects from their local LRU cache without hitting PostgreSQL.
PGJsonbStorage uses a pure-Python `OrderedDict`; RelStorage uses a
Cython-compiled generational LRU. Despite the implementation difference,
PGJsonbStorage's simpler cache structure is faster for single-key lookups.

### Loads (uncached — DB round-trip)

| Operation | PGJsonb | RelStorage | Comparison |
|---|---|---|---|
| load uncached | 97 us | 45 us | 2.2x slower |

Uncached loads force a DB round-trip on every iteration (each load accesses
a unique OID never seen before). PGJsonbStorage is slower because it must
transcode JSONB→pickle via the Rust codec after the SQL SELECT. RelStorage
returns raw bytea bytes with no post-processing. This is the expected
trade-off for storing data as queryable JSONB.

## ZODB.DB (through object cache)

| Operation | PGJsonb | RelStorage | Comparison |
|---|---|---|---|
| write simple | 5.4 ms | 5.7 ms | on par |
| write btree | 4.5 ms | 5.5 ms | **1.2x faster** |
| cached read | 2 us | 2 us | on par |
| connection cycle | 171 us | 148 us | on par |
| write batch 10 | 7.7 ms | 6.7 ms | 1.1x slower |

Through ZODB.DB, the object cache handles the hot read path — both storages
show identical 2 us reads (Python dict lookup in ZODB.Connection, not a
storage operation). Writes are on par. BTree writes are faster for
PGJsonbStorage due to efficient BTree state encoding. Batch-10 writes are
slightly slower due to JSONB overhead accumulating across multiple objects.

## Pack / GC (3 iterations, mean +/- stddev)

| Objects | PGJsonb | RelStorage | Comparison |
|---|---|---|---|
| 100 | 5.7 ms +/- 36 us | 125.4 ms +/- 4.0 ms | **22.1x faster** |
| 1,000 | 6.9 ms +/- 206 us | 163.1 ms +/- 2.3 ms | **23.5x faster** |
| 10,000 | 26.6 ms +/- 7.3 ms | 484.8 ms +/- 16.2 ms | **18.2x faster** |

Pack is the standout advantage. PGJsonbStorage's pure SQL graph traversal
via the pre-extracted `refs` column (recursive CTE) runs entirely inside
PostgreSQL — no objects are loaded, no Python unpickling occurs. RelStorage
must load and unpickle every object to discover references via
`referencesf()`. The advantage is 18-24x across all database sizes.

## Plone Operations (50 documents)

| Operation | PGJsonb | RelStorage | Comparison |
|---|---|---|---|
| site creation | 1.09 s | 1.06 s | on par |
| content create/doc | 28.2 ms | 27.2 ms | on par |
| catalog query | 199 us | 180 us | on par |
| content modify/doc | 6.8 ms | 6.7 ms | on par |

Real Plone workloads (site creation, content CRUD, catalog queries) show
both backends performing identically. This is expected: at the application
level, ZODB's object cache handles the hot path, and the per-object
transcoding cost is negligible relative to Plone's own processing
(ZCML, security, catalog indexing, event subscribers).

## Analysis

**Strengths:**
- Single-object writes 1.3x faster (simpler 2PC)
- Cached loads 3.7-4.4x faster (simpler cache structure)
- Pack/GC 18-24x faster (pure SQL, no object loading)
- BTree writes 1.2x faster
- All ZODB data queryable via SQL/JSONB (unique to PGJsonbStorage)

**Trade-offs:**
- Uncached loads 2.2x slower (JSONB→pickle transcode overhead)
- Batch store 100 is 1.2x slower (JSONB indexing overhead vs raw bytea)
- Batch ZODB write 10 is 1.1x slower

**Unique value proposition:**
PGJsonbStorage trades a small uncached-load overhead for full SQL
queryability of all ZODB data via PostgreSQL JSONB. In production, the
object cache handles >95% of reads, making the uncached-load cost largely
irrelevant. Real Plone workloads confirm zero performance difference
at the application level.

## Running Benchmarks

```bash
cd sources/zodb-pgjsonb

# Requires PostgreSQL on localhost:5433 with benchmark databases:
#   createdb -h localhost -p 5433 -U zodb zodb_bench_pgjsonb
#   createdb -h localhost -p 5433 -U zodb zodb_bench_relstorage

# Install benchmark dependencies (includes RelStorage for comparison)
uv pip install -e ".[bench]"

# For Plone benchmarks, also install Plone:
uv pip install -c example/constraints.txt Plone

# Individual benchmark categories
python benchmarks/bench.py storage --iterations 100
python benchmarks/bench.py zodb --iterations 100
python benchmarks/bench.py pack
python benchmarks/bench.py plone --docs 50

# All benchmarks with JSON export
python benchmarks/bench.py all --output benchmarks/results.json --format both
```
